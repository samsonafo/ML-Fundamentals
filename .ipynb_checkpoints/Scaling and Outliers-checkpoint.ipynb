{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling and Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Definition\n",
    "\n",
    "***Scaling***, ***standardizing*** and ***normalizing*** are used somewhat interchangeably, even though they are ***not the same thing***.\n",
    "\n",
    "True, ***standardizing*** and ***normalizing*** can be seen as particular ways of ***scaling***. Let's sort the differences between all them before proceeding.\n",
    "\n",
    "### 1.1 Scaling vs Standardizing vs Normalizing\n",
    "\n",
    "According to the implementations in Scikit-Learn library:\n",
    "\n",
    "1. ***Scaling (MinMaxScaler)***:\n",
    "\n",
    "    Transforms features by scaling each feature to a given range.\n",
    "\n",
    "    This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\n",
    "\n",
    "\n",
    "2. ***Standardizing (StandardScaler)***:\n",
    "\n",
    "    Standardize features by removing the mean and ***scaling to unit variance\n",
    "\n",
    "    The standard score of a sample x is calculated as:\n",
    "\n",
    "        z = (x - u) / s\n",
    "\n",
    "    where u is the mean of the training samples or zero if with_mean=False, and s is the standard deviation of the training samples or one if with_std=False.\n",
    "        \n",
    "        \n",
    "3. ***Normalizing (Normalizer)***:\n",
    "\n",
    "    Normalize samples individually to unit norm.\n",
    "\n",
    "    Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one.\n",
    "        \n",
    "The main difference is: ***scaling*** and ***standardizing*** operate on ***features*** (columns of your dataset), while ***normalizing*** operates on ***individual samples*** (rows on your dataset).\n",
    "\n",
    "Even though it is very common for people to say *\"you need to normalize on your features\"*, they usually mean ***standardize your features***.\n",
    "\n",
    "Why ***standardize*** instead of simply ***scaling***? It turns out, ***scaling*** is fine when you have features with a ***limited range***, like ***age*** (0-120) or ***pixel values*** (0-255), but it is likely a bad choice whenever your features may have values ***very far apart***, like ***salaries*** (maybe not yours and mine, but think of some CEOs...).\n",
    "\n",
    "### 1.2 Why Scaling?\n",
    "\n",
    "Most Machine Learning algorithms use ***gradient descent*** to learn the weights of the model. You'll see in the next lesson that ***scaling*** the features makes a ***BIG*** difference in performance.\n",
    "\n",
    "It is also important for other techniques, like ***Principal Component Analysis (PCA)*** for dimensionality reduction, and for identifying ***outliers***.\n",
    "\n",
    "![](https://imgs.xkcd.com/comics/log_scale.png)\n",
    "<center>Source: <a href=\"https://xkcd.com/1162/\">XKCD</a></center>\n",
    "\n",
    "### 1.3 Outliers\n",
    "\n",
    "What is an ***outlier***? It could be defined in several ways:\n",
    "\n",
    "- a point that is distant from the others (again, think of salaries, everyone is between USD 30k and 100k per year, and a CEO is making 50M!)\n",
    "- a point that is distinct from the others (think of black sheep in a flock of white sheep)\n",
    "- an error of measurement (think of someone listed as being 450 years old)\n",
    "- an anomaly / fraud (think of finding the purchase of a USD 10k Rolex on your credit card bill)\n",
    "\n",
    "The last case, anomalies / frauds, is a special case where ***your goal is to detect the outlier***.\n",
    "\n",
    "For now, let's focus on the other cases: in all of them, the ***presence of an outlier*** may ***hurt your model***, impacting its training and making its predictions less useful.\n",
    "\n",
    "So, how do you ***detect*** and ***remove or fence outliers***?\n",
    "\n",
    "#### 1.3.1 Tukey's Fences\n",
    "\n",
    "This is a very straightforward way of detecting ***possible*** outliers based on the ***InterQuartile Range (IQR)***.\n",
    "\n",
    "It defines a ***lower*** and an ***upper fence***, which are given by:\n",
    "\n",
    "$$\n",
    "IQR = Q_3 - Q_1\n",
    "\\\\\n",
    "lower fence = Q1 - k * IQR\n",
    "\\\\\n",
    "upper fence = Q3 + k * IQR\n",
    "$$\n",
    "\n",
    "Typical values for ***k*** are 1.5 (outlier) and 3.0 (far out).\n",
    "\n",
    "The plot below illustrates this:\n",
    "\n",
    "![](http://www.physics.csbsju.edu/stats/complex.box.defs.gif)\n",
    "\n",
    "<center>Source: <a href=\"http://www.physics.csbsju.edu/stats/box2.html\">Box Plot: Display of Distribution</a></center>\n",
    "\n",
    "Although easy to compute, ***Tukey's Fences*** only consider the distribution of a ***single feature*** to assess values as outliers or not.\n",
    "\n",
    "What if we wanted to check if a value can be considered an outlier, ***given its many features***?\n",
    "\n",
    "#### 1.3.2 Mahalanobis Distance\n",
    "\n",
    "In a single dimension, we can easily compute how far (in standard deviations) a point is from the mean. This is what ***standardization*** does for a single feature (refer to the previous lesson for more details).\n",
    "\n",
    "Mahalanobis Distance is the generalization of the same idea in multiple dimensions.\n",
    "\n",
    "The ***Mahalanobis Distance*** is given by:\n",
    "\n",
    "$$\n",
    "\\large\\sqrt{(x_1 - x_2)\\ S^{-1}\\ (x_1 - x_2)}\n",
    "$$\n",
    "\n",
    "where ***S*** is the covariance matrix.\n",
    "\n",
    "If we ***standardize*** all features, the ***Mahalanobis Distance*** corresponds to the distance from the origin:\n",
    "\n",
    "$$\n",
    "\\large\\sqrt{x\\ S^{-1}\\ x}\n",
    "$$\n",
    "\n",
    "Knowing the distance is not enough, though. To determine if a given point is an ***outlier*** or not, we need to compare its computed distance to the ***cumulative chi-squared distribution*** using the ***number of features*** as ***degrees of freedom***. If it falls ***above a threshold***, like 99.9%, it is considered an ***outlier***.\n",
    "\n",
    "```python\n",
    "from scipy.stats import chi2\n",
    "\n",
    "chi2.cdf(mahalanobis_distance, df=n_features)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
