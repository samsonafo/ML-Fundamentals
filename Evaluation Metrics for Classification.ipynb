{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Definition\n",
    "\n",
    "There are ***MANY*** metrics available for classification problems. It may be a bit ***confusing*** at first, so let's look at the ***confusion matrix*** to understand it better (pun intended!).\n",
    "\n",
    "### 1.1 Confusion Matrix\n",
    "\n",
    "The ***confusion matrix*** is the contingency table of ***actual*** (rows) vs ***predicted*** (columns) ***classes***.\n",
    "\n",
    "Some representations start with positive samples on both first row and columns. But ***Scikit-Learn*** results are returned with ***negative samples first***. So, we're sticking with its convention to avoid confusion!\n",
    "\n",
    "Therefore, a matrix has 4 values, as shown in the picture:\n",
    "\n",
    "![](./img/confusion_matrix.png)\n",
    "\n",
    "The confusion matrix provides the necessary information to build a lot of different metrics.\n",
    "\n",
    "&nbsp; | &nbsp;\n",
    ":---:|:---:\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/264px-Precisionrecall.svg.png) | ![](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Sensitivity_and_specificity.svg/264px-Sensitivity_and_specificity.svg.png)\n",
    "<center>Source: Wikipedia</center> | <center>Source: Wikipedia</center>\n",
    "\n",
    "Notice that the matrix is built on top of ***predicted classes***, not ***probabilities***. It means you should first decide on a ***threshold*** to convert probabilities into classes and only then compute the matrix.\n",
    "\n",
    "Changing the ***threshold*** will change the matrix and, consequently, the metrics that depend on its values.\n",
    "\n",
    "So, it is possible to ***tweak the threshold*** to achieve a better performance on a given metric.\n",
    "\n",
    "### 1.2 Accuracy\n",
    "\n",
    "***How often my classifier is right?***\n",
    "\n",
    "This is the most straightforward metric of all - how often a classifier is right, generally speaking.\n",
    "\n",
    "It may be a ***misleading*** metric, though, if the dataset is ***imbalanced***.\n",
    "\n",
    "$$\n",
    "Accuracy = \\frac{TP + TN}{Total}\n",
    "$$\n",
    "\n",
    "### 1.3 Precision\n",
    "\n",
    "***My classifier says it's positive - how often is it right?***\n",
    "\n",
    "If ***False Positives*** are a ***problem***, this is the metric you should pay attention to.\n",
    "\n",
    "Example: if you want to classify videos as ***appropriate for kids*** (positive) or not (negative), you ***really*** don't want a ***false positive***, that is, an ***inappropriate video*** showing up. You will end up ***rejecting good videos***, but that's a lesser problem.\n",
    "\n",
    "$$\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "### 1.4 True Positive Rate (TPR) / Recall / Sensitivity\n",
    "\n",
    "***It IS a positive sample - how often my classifier gets it right?***\n",
    "\n",
    "If ***False Negatives*** are a ***problem***, this is the metric you should pay attention to.\n",
    "\n",
    "Example: if you want to detect if someone has a ***rare and fatal disease*** (positive) or not (negative), you ***really*** don't want a ***false negative***, that is, ***dismissing a sick person***. You will end up ***investigating further healthy people***, but that's a lesser problem.\n",
    "\n",
    "$$\n",
    "Recall = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "### 1.5 False Positive Rate (FPR) / Specificity\n",
    "\n",
    "***It IS a negative sample - how often my classifier gets it wrong?***\n",
    "\n",
    "If ***False Positives*** are a ***problem***, this is the metric you should pay attention to.\n",
    "\n",
    "$$\n",
    "FPR = 1 - Specificity = 1 - \\frac{TN}{TN + FP} = \\frac{FP}{TN + FP}\n",
    "$$\n",
    "\n",
    "### 1.6 F1-Score\n",
    "\n",
    "It is the ***harmonic mean*** of precision and recall, so it combines both metrics into a single value.\n",
    "\n",
    "It favors classifiers that deliver similar levels of precision and recall.\n",
    "\n",
    "$$\n",
    "F_1 = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}}\n",
    "$$\n",
    "\n",
    "### Tweaking the Threshold\n",
    "\n",
    "The metrics so far were computed for a given threshold. If we want to compare how they fare whenever we ***change the threshold*** to all its possible values, we need to construct one of these ***curves*** below.\n",
    "\n",
    "They are especially useful to evaluate classifiers on ***imbalanced datasets***.\n",
    "\n",
    "### 1.7 Precision-Recall Curve (Recall x Precision)\n",
    "\n",
    "The ***PR Curve*** depicts the trade-off between ***Recall*** on the horizontal axis and ***Precision*** on the vertical axis.\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_precision_recall_001.png)\n",
    "<center>Source: Scikit-Learn</center>\n",
    "\n",
    "You may have noticed the curve is somewhat ***bumpy***.\n",
    "\n",
    "If you ***raise the threshold***, you will move to the ***left*** on the curve. \n",
    "\n",
    "It means you're trying to ***avoid False Positives*** at the expense of ***trading True Positives for False Negatives***.\n",
    "1. More FN reduces Recall (TPR) (less TP has little impact as its on both numerator and denominator)\n",
    "2. Less FP increases precision, but less TP reduces precision\n",
    "\n",
    "But, as you shift the threshold, you may ***lose more TPs than FPs***, and then it will reduce your precision momentarily.\n",
    "\n",
    "### 1.8 ROC Curve (FPR x TPR)\n",
    "\n",
    "The ***ROC Curve*** depicts the trade-off between ***False Positive Rate*** on the horizontal axis and ***True Positive Rate*** on the vertical axis.\n",
    "\n",
    "The shape of the curve will depend on ***how separable*** the classes are:\n",
    "- perfectly separable: the \"curve\" would actually be a square, going straight up to 1 and staying there\n",
    "- completely overlapped: the \"curve\" would actually be a diagonal line, from the origin to the upper right corner\n",
    "- somewhat separable: a curve like the one in the figure below\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_roc_001.png)\n",
    "<center>Source: Scikit-Learn</center>\n",
    "\n",
    "If you ***raise the threshold***, you will move to the ***left*** on the curve. \n",
    "\n",
    "It means you're trying to ***avoid False Positives*** at the expense of ***trading True Positives for False Negatives***.\n",
    "1. More FN reduces TPR (Recall) (less TP has little impact as its on both numerator and denominator)\n",
    "2. Less FP reduces FPR\n",
    "\n",
    "Since TP is not present on the calculation of FPR, we ***do not*** observe the bumpiness as in the PR Curve.\n",
    "\n",
    "### 1.9 Area Under ROC\n",
    "\n",
    "The ROC Curve is a very popular method of evaluating a binary classifier. But how does one compare two curves? Unless one of them is strictly better than the other, this would be a difficult task.\n",
    "\n",
    "To make it easier to compare classifiers, one can use the ***area*** under the ROC Curve. The closer it is to ***one***, the better the classifier, as it achieves a high ***TPR*** with a little ***FPR***.\n",
    "\n",
    "üîπ Intuitive Meaning\n",
    "\n",
    "AUC = the probability that your model ranks a random positive higher than a random negative.\n",
    "\n",
    "Imagine you randomly pick 1 positive example (say, a real spam email) and 1 negative example (a normal email).\n",
    "\n",
    "You look at your model‚Äôs predicted probability for each.\n",
    "\n",
    "If the spam gets a higher score ‚Üí that‚Äôs a ‚Äúwin‚Äù for the model.\n",
    "\n",
    "If the normal email gets a higher score ‚Üí that‚Äôs a ‚Äúloss.‚Äù\n",
    "\n",
    "Do this many times ‚Äî the fraction of ‚Äúwins‚Äù is the AUC.\n",
    "\n",
    "So if AUC = 0.85, it means that 85% of the time, the model ranks a random positive higher than a random negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
