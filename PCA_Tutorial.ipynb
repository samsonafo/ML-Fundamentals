{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8329763d",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a technique used for **dimensionality reduction** in data science and machine learning.\n",
    "It helps simplify data, visualize patterns, and improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcbf3ed",
   "metadata": {},
   "source": [
    "## Why Use PCA?\n",
    "- To **reduce the number of features** while preserving as much variance as possible.\n",
    "- To **visualize high-dimensional data** in 2D or 3D.\n",
    "- To **remove multicollinearity** in datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff78362e",
   "metadata": {},
   "source": [
    "## How PCA Works (Intuition)\n",
    "1. Standardize the data.\n",
    "2. Compute the covariance matrix.\n",
    "3. Compute the eigenvectors and eigenvalues.\n",
    "4. Sort eigenvectors by eigenvalues in descending order.\n",
    "5. Choose the top *k* eigenvectors and project the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5491d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: PCA on Iris Dataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n",
    "df_pca['Target'] = y\n",
    "\n",
    "# Plot the PCA result\n",
    "plt.figure(figsize=(8,6))\n",
    "for i in range(3):\n",
    "    plt.scatter(df_pca[df_pca['Target']==i]['PC1'], df_pca[df_pca['Target']==i]['PC2'], label=iris.target_names[i])\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of Iris Dataset')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f3c0a0",
   "metadata": {},
   "source": [
    "## Visual Guide to PCA\n",
    "The diagram below shows how PCA finds new axes (principal components):\n",
    "\n",
    "![PCA visualization](./img/pca_img.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91fc81d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- PCA reduces dimensions by finding new axes of maximum variance.\n",
    "- Itâ€™s commonly used for visualization and preprocessing.\n",
    "- The main tools are standardization, covariance matrix, eigenvectors.\n",
    "\n",
    "ðŸŽ¯ Now you understand how PCA works and how to apply it using scikit-learn!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
